---
title: "Comparison Parameters"
author: "Leoni Zimmermann"
date: 28/08/2024
format:
  html:
    code-fold: true
    code-summary: "Show the code"
    toc: true
editor: visual
---

```{r message=FALSE, warning=FALSE, include=FALSE}
suppressMessages(c(library(tidyverse),
                   library(plotly),
                   library(patchwork),
                   library(knitr)))
```

# Summary

This lab rotation project compares two methods for multi-omics data integration and imputation: MOFA (Multi-Omics Factor Analysis) and StabMap. I evaluate their performance on various metrics. I focus on silhouette scores and cell type classification accuracy for integration, and root mean square error (RMSE) for imputation. The first part of the analysis explores which parameters are best suited for integration and imputation, respectively. The effect of different bridge sizes (overlap of features between the datasets) is discussed in the second part.

# First part: parameter selection

## Functions

Custom functions for improved readability

```{r, message=FALSE, warning=FALSE, code-fold=TRUE}
mofa_plot_metric_for_diff_factors <- function(data, parameter, metric, col = NA, ...) {
  p <- data %>% 
    group_by({{parameter}}) %>% 
    mutate(mean = mean({{metric}})) %>%
    ggplot() + 
      geom_point(aes(x = {{parameter}}, y = mean), col = "red") +
      geom_line(aes(x = {{parameter}}, y = mean, group = 1), col = "red") +
      theme_classic() +
      labs(...)

  if (length(col) > 1) {
    p <- p + geom_point(aes(x = {{parameter}}, y = {{metric}}, col = {{col}}), alpha = 0.25)
    p <- p + guides(col = guide_legend(title = gsub(".*\\$", "", deparse(substitute(col)))))
  } else {
    p <- p + geom_point(aes(x = {{parameter}}, y = {{metric}}), alpha = 0.5, col = "gray")
  }
  return(p)
}

stab_plot_metric_for_diff_factors <- function(data, metric, col = NA, ...) {
  p <- data %>% 
    group_by(ncomponentsReference, ncomponentsSubset) %>% 
    mutate(mean = mean({{metric}})) %>%
    ggplot() + 
      geom_point(aes(x = ncomponentsReference, y = mean), col = "red") +
      geom_line(aes(x = ncomponentsReference, y = mean, group = 1), col = "red") +
      facet_grid(cols = vars(ncomponentsSubset)) +
      theme_classic() +
      labs(...)
  if (length(col) > 1) {
    p <- p + geom_point(aes(x = ncomponentsReference, y = {{metric}}, col = {{col}}), alpha = 0.25)
    p <- p + guides(col = guide_legend(title = gsub(".*\\$", "", deparse(substitute(col)))))
  } else {
    p <- p + geom_point(aes(x = ncomponentsReference, y = {{metric}}), alpha = 0.5, col = "gray")
  }
  return(p)
}

plot_metric_trend <- function(data, x, y) {
  ggplot(data, aes(x = {{x}}, y = {{y}})) +
  geom_point() +
  geom_line(group = 1) +
  theme_classic()
} 

compare_methods <- function(mofa_data, mofa_param, mofa_metric, stab_data, stab_param, stab_metric, ...) {
  ggplot() +
    geom_point(data = mofa_data,aes( x = {{mofa_param}}, y = {{mofa_metric}}, col = "MOFA")) + 
    geom_line(data = mofa_data,aes(x = {{mofa_param}}, y = {{mofa_metric}}, group = 1, col = "MOFA")) +
    geom_point(data = stab_data,aes(x = {{stab_param}}, y = {{stab_metric}}, col = "StabMap")) +
    geom_line(data = stab_data,aes(x = {{stab_param}}, y ={{stab_metric}}, group = 1, col = "StabMap")) +
    labs(...) + 
    theme_classic()
}

stability_rand_NAcells_barplot <- function(data, mofa_metric, mofa_sd, stab_metric, stab_sd, ...) {
  ggplot(data) +
    geom_col(aes(x = 1, y = {{mofa_metric}}, fill = "MOFA"), width = 0.2) +
    geom_errorbar(aes(x = 1, ymin = {{mofa_metric}} - {{mofa_sd}}, ymax = {{mofa_metric}} + {{mofa_sd}}), width = 0.05) +
    geom_col(aes(x = 1.25, y = {{stab_metric}}, fill = "StabMap"), width = 0.2) +
    geom_errorbar(aes(x = 1.25, ymin = {{stab_metric}} - {{stab_sd}}, ymax = {{stab_metric}} + {{stab_sd}}), width = 0.05) +
    theme_classic() +
    theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +
    labs(x = "", fill = "Tool", ...) 
} 

```

## Data

```{r}
# MOFA
mofa_param_comparison <- read.table("~/R/Data/mofa_comparison.txt", header = TRUE) 

mofa_comparison_set_param  <- read.table("~/R/Data/mofa_comparison_numfactors.txt", header = TRUE)

mofa_rand_NAcells_70 <- read.table("~/R/Data/mofa_random_cells_NA_70.txt", header = TRUE)
mofa_rand_NAcells <- read.table("~/R/Data/mofa_random_cells_NA_15.txt", header = TRUE) %>%
  select(-rmse) %>%
  cbind(rmse = mofa_rand_NAcells_70$rmse)


# StabMap
stab_param_comparison <- read.table("~/R/Data/stab_comparison.txt", header = TRUE)
stab_comparison_set_param  <- read.table("~/R/Data/stab_comparison_numfactors.txt") %>%
  mutate(num_pc = ncomponentsReference)

stab_rand_NAcells <- read.table("~/R/Data/stab_random_cells_NA.txt", header = TRUE) 
```

## Finding the parameters maximizing selected metrics

Data generated by `MOFA_parameters.R` and `StabMap_parameters.R`

### Number of factors/PC

Integration and imputation were performed with various combinations of different parameter ranges. The plots illustrate the trend of the three selected metrics for varying numbers of factors/principal components (PC). The metric was averaged over all runs with the same number of factors/PCs. The gray dots represent the individual values.

```{r}
# MOFA
print(mofa_plot_metric_for_diff_factors(mofa_param_comparison, num_factor, mean_sil_score, col = mofa_param_comparison$scale_views, 
                                        title = "Mean silhouette score for different number of factors", x = "Number of factors", y = "Mean silhouette score") +
      mofa_plot_metric_for_diff_factors(mofa_param_comparison, num_factor, mofa_knn_acc_bal, 
                                        title = "Mean mofa_knn_acc_bal for different number of factors", x = "Number of factors", y = "Mean celltype accuracy") +  
      mofa_plot_metric_for_diff_factors(mofa_param_comparison, num_factor, mofa_rmse, 
                                        title = "Mean RMSE for different number of factors", x = "Number of factors", y = "Mean RMSE") +
      plot_annotation(title = 'Influence of different number of factors for integration and imputation with MOFA'))

# StabMap
print(stab_plot_metric_for_diff_factors(stab_param_comparison, mean_sil_score, col = stab_param_comparison$scale.scale, 
                                        title = "Mean silhouette score for different number of components", x = "Number of reference components", y = "Mean silhouette score") +
      stab_plot_metric_for_diff_factors(stab_param_comparison, stab_knn_acc_bal, 
                                        title = "Mean stab_knn_acc_bal for different number of components", x = "Number of reference components", y = "Mean stab_knn_acc_bal") +
      stab_plot_metric_for_diff_factors(stab_param_comparison, stab_rmse, 
                                        title = "Mean RMSE for different number of components", x = "Number of reference components", y = "Mean RMSE") +
      plot_annotation(title = 'Influence of different number of components for integration and imputation with StabMap'))
```

I observed that fewer factors resulted in improved integration outcomes for MOFA, with 15 factors demonstrating the most optimal performance. Conversely, for StabMap, I identify a higher number of PC as the optimal parameter. Imputation exhibited the most favorable outcomes for both methods with the highest number of tested factors/PC.

### Other parameters

To assess the effect of the other parameters, I repeat the same plot for all possible metrics. (The HTML output is disabled since it comprises a considerable number of plots.)

```{r, output = FALSE}
# MOFA
for (j in 1:3) {
  par_name <- rlang::sym(colnames(mofa_param_comparison)[j])
  for (i in 1:9) {
    var_name <- rlang::sym(colnames(mofa_param_comparison)[3+i])
    print(mofa_plot_metric_for_diff_factors(mofa_param_comparison, !!par_name, !!var_name, 
                                            title = paste("Mean", var_name, "for different number of components"), x = par_name, y = paste("Mean", var_name)))
  }
}


# StabMap
for (j in 1:6) {
  par_name <- rlang::sym(colnames(stab_param_comparison)[j])
  for (i in 1:6) {
    var_name <- rlang::sym(colnames(stab_param_comparison)[6+i])
    print(mofa_plot_metric_for_diff_factors(stab_param_comparison, !!par_name, !!var_name, 
                                            title = paste("Mean", var_name, "for different number of components"), x = par_name, y = paste("Mean", var_name)))
  }
}
```

Results for MOFA:

| Parameter         | Integration                              | Imputation                                               | Optimal setting |
|--------------|------------------|--------------------------|--------------|
| scale_views       | Slightly better when FALSE               | Slightly better when TRUE, but insignificant difference  | FALSE           |
| spikeslab_weights | Mixed results, overall better when FALSE | Slightly better when FALSE, but insignificant difference | FALSE           |

Results for StabMap:

| Parameter    | Integration              | Imputation               | Optimal setting                                                        |
|--------------|--------------|--------------|--------------------------------|
| PC Subset    | Insignificant difference | Insignificant difference | Same number as PC reference (since both have the same amount of cells) |
| maxFeatures  | \-                       | \-                       | Not analyzed since feature selection is done before.                   |
| scale.center | Better when TRUE         | Better when TRUE         | FALSE                                                                  |
| scale.scale  | Better when FALSE        | Better when FALSE        | FALSE                                                                  |
| project_all  | Better when TRUE         | Better when FALSE        | TRUE (to keep it comparable with MOFA)                                 |

I set all scaling parameters to FALSE to keep both methods comparable.

Another way to visualize:\

```{r}
# MOFA
metrics <- c("mean_sil_score", "mofa_rmse", "mofa_knn_acc_bal")

for (j in 1:3) {
  metric <- metrics[j]
  for (i in c(1,3)) {
    var_name <- colnames(mofa_param_comparison)[i]
    
    plot_data <- mofa_param_comparison %>%
      group_by(num_factor, !!sym(var_name)) %>%
      summarize(mean = mean(!!sym(metric)), .groups = 'drop')
    
    p <- ggplot(plot_data) +
      geom_point(aes(x = num_factor, 
                     y = mean, 
                     col = as.factor(!!sym(var_name)))) +
      geom_line(aes(x = num_factor, 
                    y = mean, 
                    group = !!sym(var_name), 
                    col = as.factor(!!sym(var_name)))) +
      theme_classic() +
      ggtitle(paste("Plot for parameter:", var_name, "- Metric:", metric)) +
      ylab(paste("Mean", metric)) +
      scale_color_discrete(name = var_name)
    
    print(p)
  }
}

# StabMap
metrics <- c("mean_sil_score", "stab_rmse", "stab_knn_acc_bal")

for (j in 1:3) {
  metric <- metrics[j]
  for (i in 1:5) {
    var_name <- colnames(stab_param_comparison)[1 + i]
    
    plot_data <- stab_param_comparison %>%
      group_by(ncomponentsReference, !!sym(var_name)) %>%
      summarize(mean = mean(!!sym(metric)), .groups = 'drop')
    
    p <- ggplot(plot_data) +
      geom_point(aes(x = ncomponentsReference, 
                     y = mean, 
                     col = as.factor(!!sym(var_name)))) +
      geom_line(aes(x = ncomponentsReference, 
                    y = mean, 
                    group = !!sym(var_name), 
                    col = as.factor(!!sym(var_name)))) +
      theme_classic() +
      ggtitle(paste("Plot for parameter:", var_name, "- Metric:", metric)) +
      ylab(paste("Mean", metric)) +
      scale_color_discrete(name = var_name)
    
    print(p)
  }
}
```

## Comparison with best results per metric

Data generated by `MOFA_parameters.R` and `StabMap_parameters.R`

In this section, I present a comparison of the metric results obtained from the runs with the parameter setting I have previously described and the best metric results from all possible parameter combinations.

```{r}
mofa_table <- mofa_param_comparison %>%
  filter(scale_views == FALSE, spikeslab_weights == FALSE, num_factor %in% c(15, 70))
  
mofa_table_2 <- data.frame(
      metric = c("mean_sil_score", "mofa_knn_acc_bal", "mofa_rmse"),
      values_selected_parameters = c(
        mofa_table$mean_sil_score[1],
        mofa_table$mofa_knn_acc_bal[1],
        mofa_table$mofa_rmse[2]),
      max_value = c(
        max(mofa_param_comparison$mean_sil_score),
        max(mofa_param_comparison$mofa_knn_acc_bal),
        min(mofa_param_comparison$mofa_rmse)),
      min_value = c(
        min(mofa_param_comparison$mean_sil_score),
        min(mofa_param_comparison$mofa_knn_acc_bal),
        max(mofa_param_comparison$mofa_rmse))) %>%
      mutate(diff = max_value - values_selected_parameters )

mofa_table_2
```

```{r}
stab_table <- stab_param_comparison %>%
  filter(scale.scale == FALSE, scale.center == FALSE, ncomponentsReference == 70, ncomponentsSubset == 70, project_all == TRUE, maxFeatures == 1740) 

stab_table_2 <- data.frame(
    metric = c("mean_sil_score", "stab_knn_acc_bal", "stab_rmse"),
    values_selected_parameters = c(
      stab_table$mean_sil_score,
      stab_table$stab_knn_acc_bal,
      stab_table$stab_rmse),
    max_value = c(
      max(stab_param_comparison$mean_sil_score),
      max(stab_param_comparison$stab_knn_acc_bal),
      min(stab_param_comparison$stab_rmse)),
    min_value = c(
      min(stab_param_comparison$mean_sil_score),
      min(stab_param_comparison$stab_knn_acc_bal),
      max(stab_param_comparison$stab_rmse))) %>%
    mutate(diff = max_value - values_selected_parameters )

stab_table_2
```

The difference between the maximum value and the values obtained with the two methods is minimal for all metrics.\

Here I compare the metric values, calculated with the above chosen parameters, of both tools against each other

```{r}
tibble(
  metric = c("mean_sil_score", "knn_acc_bal", "RMSE"),
  MOFA = mofa_table_2$values_selected_parameters,
  StabMap = stab_table_2$values_selected_parameters
)
```

MOFA has better results for imputation and celltype accuracy, StabMap at the silhouette width.

## Metrics for different num of factors with set parameters

Data generated by `MOFA_parameters.R` and `StabMap_parameters.R`

Here I visualize how the number of factors/PC affects the results as the only varying parameter.

```{r}
# MOFA
plot_metric_trend(mofa_comparison_set_param , num_factor, mofa_rmse)
plot_metric_trend(mofa_comparison_set_param , num_factor, mean_sil_score)
plot_metric_trend(mofa_comparison_set_param , num_factor, mofa_knn_acc_bal)

# StabMap
plot_metric_trend(stab_comparison_set_param , num_pc, stab_rmse)
plot_metric_trend(stab_comparison_set_param , num_pc, mean_sil_score)
plot_metric_trend(stab_comparison_set_param , num_pc, stab_knn_acc_bal)
```

This verifies that the optimal number of factors for integration is 15 (20 would be another option), and 70 for imputation. However, the RMSE does not exhibit a notable reduction when the number of factors exceeds 30. For StabMap 70 PCs for both the reference and query are optimal.

The following figures depict a comparison of the metric trends between MOFA and StabMap:

```{r}
compare_methods(mofa_comparison_set_param , mofa_param = num_factor, mofa_metric = mean_sil_score, 
                stab_comparison_set_param , stab_param = num_pc, stab_metric = mean_sil_score,
                x = "Number factors/PC", y = "Mean silhouette score", colour = "Tool")

compare_methods(mofa_comparison_set_param , mofa_param = num_factor, mofa_metric = mofa_knn_acc_bal, 
                stab_comparison_set_param , stab_param = num_pc, stab_metric = stab_knn_acc_bal,
                x = "Number factors/PC", y = "Celltype accuracy", colour = "Tool")

compare_methods(mofa_comparison_set_param , mofa_param = num_factor, mofa_metric = mofa_rmse, 
                stab_comparison_set_param , stab_param = num_pc, stab_metric = stab_rmse,
                x = "Number factors/PC", y = "RMSE", colour = "Tool")
```

## Random NA cells

Data generated by `random_cells_NA.R`

To assess the stability of the results, I repeated integration and imputation five times with different cells having ATAC values missing. (To not be confused: for MOFA 15 factors were used for integration metrics and 70 factors for imputation)

```{r}
random_NA_both <- cbind(
                    stab_rand_NAcells %>%
                    summarize(
                      stab_mean_sil_score_mean = mean(mean_sil_score), 
                      stab_mean_sil_score_sd = sd(mean_sil_score),
                      stab_knn_acc_bal_mean = mean(knn_acc_bal), 
                      stab_knn_acc_bal_sd = sd(knn_acc_bal), 
                      stab_rmse_mean = mean(rmse), 
                      stab_rmse_sd = sd(rmse)
                    ) ,
                    mofa_rand_NAcells %>%
                    summarize(
                      mofa_mean_sil_score_mean = mean(mean_sil_score), 
                      mofa_mean_sil_score_sd = sd(mean_sil_score),
                      mofa_knn_acc_bal_mean = mean(knn_acc_bal), 
                      mofa_knn_acc_bal_sd = sd(knn_acc_bal), 
                      mofa_rmse_mean = mean(rmse), 
                      mofa_rmse_sd = sd(rmse)
                    ) 
                  )
```

```{r}
stability_rand_NAcells_barplot(random_NA_both, 
                  mofa_metric = mofa_mean_sil_score_mean,
                  mofa_sd = mofa_mean_sil_score_sd,
                  stab_metric = stab_mean_sil_score_mean,
                  stab_sd = stab_mean_sil_score_sd, 
                  y = "Mean silhouette width")

stability_rand_NAcells_barplot(random_NA_both, 
                  mofa_metric = mofa_knn_acc_bal_mean *100,
                  mofa_sd = mofa_knn_acc_bal_sd*100,
                  stab_metric = stab_knn_acc_bal_mean*100,
                  stab_sd = stab_knn_acc_bal_sd*100, 
                  y = "Mean Celltype Accuracy [%]")

stability_rand_NAcells_barplot(random_NA_both, 
                  mofa_metric = mofa_rmse_mean,
                  mofa_sd = mofa_rmse_sd,
                  stab_metric = stab_rmse_mean,
                  stab_sd = stab_rmse_sd, 
                  y = "Mean RMSE")
```

The goodness of the results does not depend on what cells are missing the ATAC features.

# Second part: different bridge sizes

The bridge are the features that are common to both datasets. Therefore the bridge size is the the number of features shared. Here I test how well integration and imputation work with reduced bridge sizes.

## Functions

```{r}
plot_bridge_sizes <- function(data, knn_acc_bal, rmse, rmse_rna, rmse_atac, seperate_rmse = TRUE) {
  plot1 <- ggplot(data) +
    geom_point(aes(x = bridge_size, y = mean_sil_score)) +
    geom_line(aes(x = bridge_size, y = mean_sil_score, group = 1)) +
    theme_classic() +
    labs(x = "Bridge size", y = "Mean silhouette score", title = "Influence of different bridge sizes on the mean silhouette score")
  
  plot2 <- ggplot(data) +
    geom_point(aes(x = bridge_size, y = {{knn_acc_bal}})) +
    geom_line(aes(x = bridge_size, y = {{knn_acc_bal}}, group = 1)) +
    theme_classic() +
    labs(x = "Bridge size", y = "Celltype accuracy", title = "Influence of different bridge sizes on the Celltype accuracy")
  
  plot3 <- ggplot(data) +
    geom_point(aes(x = bridge_size, y = {{rmse}})) +
    geom_line(aes(x = bridge_size, y = {{rmse}}, group = 1)) +
    theme_classic() +
    labs(x = "Bridge size", y = "RMSE", title = "Influence of different bridge sizes on the RMSE")
  
  if (seperate_rmse) {
    plot4 <- ggplot(data) +
    geom_point( aes(x = bridge_size, y = {{rmse}})) +
    geom_line( aes(x = bridge_size, y = {{rmse}}, group = 1)) +
    geom_point( aes(x = bridge_size, y = {{rmse_rna}}, col = "RNA")) +
    geom_point( aes(x = bridge_size, y = {{rmse_atac}}, col = "ATAC")) +
    theme_classic() +
    labs(x = "Bridge size", y = "RMSE", title = "Influence of different bridge sizes on the RMSE (with RNA and ATAC seperated)")
    
    return(list(sil_score = plot1, celltype_accuracy = plot2, rmse = plot3, rmse_rna_atac_sep = plot4))
  }
  
  return(list(sil_score = plot1, celltype_accuracy = plot2, rmse = plot3))
}

stability_rand_NAcells_lineplot <- function(data) {
  plot1 <- ggplot(data) +
    geom_point(aes(x = bridge_size, y = mean_sil_score_mean)) +
    geom_errorbar(aes(x =  bridge_size, ymin = mean_sil_score_mean - mean_sil_score_sd, ymax = mean_sil_score_mean + mean_sil_score_sd), width = 0.05)+
    geom_line(aes(x =  bridge_size, y = mean_sil_score_mean, group = 1)) +
    theme_classic() +
    xlab("bridge_size")
  
  plot2 <- ggplot(data) +
    geom_point(aes(x = bridge_size, y = knn_acc_bal_mean)) +
    geom_errorbar(aes(x = bridge_size, ymin = knn_acc_bal_mean - knn_acc_bal_sd, ymax = knn_acc_bal_mean + knn_acc_bal_sd), width = 0.05)+
    geom_line(aes(x =  bridge_size, y = knn_acc_bal_mean, group = 1)) +
    theme_classic() +
    xlab("bridge_size")

  plot3 <- ggplot(data) +
    geom_point(aes(x = bridge_size, y = rmse_mean)) +
    geom_errorbar(aes(x = bridge_size, ymin = rmse_mean - rmse_sd, ymax = rmse_mean + rmse_sd), width = 0.05)+
    geom_line(aes(x =  bridge_size, y = rmse_mean, group = 1)) +
    theme_classic() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))  +
    xlab("bridge_size")

  return(list(sil_score = plot1, celltype_accuracy = plot2, rmse = plot3))
}
```

## Data

```{r}
mofa_bridge_70 <- read.table("~/R/Data/mofa_bridge_70.txt", header = TRUE)
mofa_bridge_15 <- read.table("~/R/Data/mofa_bridge_15.txt", header = TRUE)
stab_bridge <- read.table("~/R/Data/stab_bridge.txt", header = TRUE)

mofa_bridge_rand_70 <- read.table("~/R/Data/mofa_bridge_70_rand.txt", header = TRUE)
mofa_bridge_rand <- read.table("~/R/Data/mofa_bridge_15_rand.txt", header = TRUE) %>%
  select(-rmse) %>%
  cbind(rmse = mofa_bridge_rand_70$rmse) 
stab_bridge_rand <- read.table("~/R/Data/stab_bridge_rand.txt", header = TRUE)

mofa_incomp_ref_70 <- read.table("~/R/Data/mofa_RNA_out_70.txt", header = TRUE)
mofa_incomp_ref_15 <- read.table("~/R/Data/mofa_RNA_out_15.txt", header = TRUE) %>%
  select(-c(rmse_ATAC, rmse_RNA)) %>%
  cbind(rmse_ATAC = mofa_incomp_ref_70$rmse_ATAC) %>%
  cbind(rmse_RNA = mofa_incomp_ref_70$rmse_RNA)
stab_incomp_ref <- read.table("~/R/Data/stab_RNA_out.txt", header = TRUE)
```

## Influence of bridge size on integration and imputation metrics

Data generated by `Bridge_analysis.R`

I ran integration and imputation for the following bridge sizes: 10, 52, 104, 156, 218, 438, 653, 870 , 953, 1088, 1305, 1523, and 1730. Given the unexpected outcome of the RMSE, I conducted a separate calculation for ATAC and RNA features to provide an explanation for the observed RMSE trend.

```{r}
plot_bridge_sizes(data = mofa_bridge_70, 
                  knn_acc_bal = knn_acc_bal, 
                  rmse = rmse,  
                  rmse_rna = rmse_rna, 
                  rmse_atac = rmse_atac)

plot_bridge_sizes(data = mofa_bridge_15,
                  knn_acc_bal = knn_acc_bal,
                  rmse = rmse,
                  seperate_rmse = FALSE)

plot_bridge_sizes(data = stab_bridge, 
                  knn_acc_bal = knn_acc_bal, 
                  rmse = rmse,  
                  rmse_rna = rmse_rna, 
                  rmse_atac = rmse_atac)
```

There is a pronounced increase in both the mean silhouette score and celltype accuracy between bridge sizes 10 and 438, for both tools. This stabilizes around a bridge size of 438, with only minor fluctuations evident at higher bridge sizes. From size 10 to 104, there is a notable decrease, indicating an improvement in performance. However, there is a subsequent and significant increase until bridge size 953, which is the point at which only ATAC features are imputed. Subsequently, the value declines, though not to the same extent as observed with bridge sizes between 104 and 653. This behavior can be explained by calculating the RMSE separately for ATAC and RNA features. The plot illustrates that the RMSE for RNA features is approximately 0.4 lower. Consequently, the RMSE decreases as the RNA features, which are easier to impute, are incorporated into the missing values.

Now I compare the results of both tolls:

```{r}
compare_methods(mofa_bridge_15, bridge_size, mean_sil_score, 
               stab_bridge, bridge_size, mean_sil_score,
               x = "Bridge size", y = "Mean silhouette score")

compare_methods(mofa_bridge_15, bridge_size, knn_acc_bal, 
               stab_bridge, bridge_size, knn_acc_bal,
               x = "Bridge size", y = "Celltype accuracy")

compare_methods(mofa_bridge_70, bridge_size, rmse, 
               stab_bridge, bridge_size, rmse,
               x = "Bridge size", y = "RMSE")
```

As noted before, MOFA outperform StabMap at both integration and imputation.

## Random NA features

Data generated by `Bridge_analysis.R`

To remove the effect that one ATAC features are harder to impute than the RNA features, I ran the models with random missing features (but same set of NA features for both tools)

```{r}
mofa_bridge_rand_summ <- mofa_bridge_rand %>% group_by(bridge_size) %>%
                    summarize(
                      mean_sil_score_mean = mean(mean_sil_score), 
                      mean_sil_score_sd = sd(mean_sil_score),
                      knn_acc_bal_mean = mean(knn_acc_bal), 
                      knn_acc_bal_sd = sd(knn_acc_bal), 
                      rmse_mean = mean(rmse), 
                      rmse_sd = sd(rmse)
                    ) 

stab_bridge_rand_summ <- stab_bridge_rand %>%
  group_by(bridge_size) %>%
                    summarize(
                      mean_sil_score_mean = mean(mean_sil_score), 
                      mean_sil_score_sd = sd(mean_sil_score),
                      knn_acc_bal_mean = mean(knn_acc_bal), 
                      knn_acc_bal_sd = sd(knn_acc_bal), 
                      rmse_mean = mean(rmse), 
                      rmse_sd = sd(rmse)
                    )

stability_rand_NAcells_lineplot(mofa_bridge_rand_summ)
stability_rand_NAcells_lineplot(stab_bridge_rand_summ)
```

Both integration metrics look similar to before. The course of the RMSE now looks more as expected than before. It improves from bridge size 10 to 156 and stays relatively consistent afterwards. MOFA exhibits a greater improvement. At a bridge size of 1730 (only 10 features are missing) the standard deviation is significantly bigger than for others. The reason could be that outliers have a bigger effect on the RMSE when the number of samples is small.

Once again a comparison between both tools:

```{r}
compare_methods(mofa_data = mofa_bridge_rand_summ, mofa_param = bridge_size, mofa_metric = mean_sil_score_mean,
                stab_data = stab_bridge_rand_summ, stab_param = bridge_size, stab_metric = mean_sil_score_mean,
                x = "bridge size", y = "Mean silhouette score")

compare_methods(mofa_data = mofa_bridge_rand_summ, mofa_param = bridge_size, mofa_metric = knn_acc_bal_mean,
                stab_data = stab_bridge_rand_summ, stab_param = bridge_size, stab_metric = knn_acc_bal_mean,
                x = "bridge size", y = "Celltype accuracy")

compare_methods(mofa_data = mofa_bridge_rand_summ, mofa_param = bridge_size, mofa_metric = rmse_mean,
                stab_data = stab_bridge_rand_summ, stab_param = bridge_size, stab_metric = rmse_mean,
                x = "bridge size", y = "RMSE")
```

MOFA once again outperforms StabMap.

## Incomplete Reference dataset

Data generated by `different_bridge_sizes_RNA_out.R`

Up to this point, the reference dataset comprised the full range of features. In this section, the effect of an incomplete reference dataset is tested. The query will still miss the ATAC features, but now the reference will start missing different numbers of (randomly selected) RNA features. RNA bridges of varying sizes are applied: 10, 56, 112, 167, 233, 476, 709, and 942 (in total 952 RNA features). Here are the trends for the three metrics:

```{r}
mofa_incomp_ref_15_summ <- mofa_incomp_ref_15 %>% group_by(bridge_size) %>%
                    summarize(
                      mean_sil_score_mean = mean(mean_sil_score), 
                      knn_acc_bal_mean = mean(knn_acc_bal),
                      rmse_ATAC_mean = mean(rmse_ATAC), 
                      rmse_RNA_mean = mean(rmse_RNA), 
                    ) 

stab_incomp_ref_summ <- stab_incomp_ref %>%
  group_by(bridge_size) %>%
                    summarize(
                      mean_sil_score_mean = mean(mean_sil_score), 
                      knn_acc_bal_mean = mean(knn_acc_bal),
                      rmse_ATAC_mean = mean(rmse_ATAC), 
                      rmse_RNA_mean = mean(rmse_RNA), 
                    )

plot_metric_trend(mofa_incomp_ref_15_summ, bridge_size, mean_sil_score_mean)
plot_metric_trend(mofa_incomp_ref_15_summ, bridge_size, knn_acc_bal_mean)
plot_metric_trend(mofa_incomp_ref_15_summ, bridge_size, rmse_ATAC_mean)
plot_metric_trend(mofa_incomp_ref_15_summ, bridge_size, rmse_RNA_mean)

plot_metric_trend(stab_incomp_ref_summ, bridge_size, mean_sil_score_mean)
plot_metric_trend(stab_incomp_ref_summ, bridge_size, knn_acc_bal_mean)
plot_metric_trend(stab_incomp_ref_summ, bridge_size, rmse_ATAC_mean)
plot_metric_trend(stab_incomp_ref_summ, bridge_size, rmse_RNA_mean)
```

As the bridge size increases, both the integration and imputation metrics demonstrate a corresponding improvement, with the former rising and the latter falling. This suggests that both tools are more effective when utilised with larger bridges.

Comparison:

```{r}
compare_methods(mofa_data = mofa_incomp_ref_15_summ, mofa_param = bridge_size, mofa_metric = mean_sil_score_mean,
                stab_data = stab_incomp_ref_summ, stab_param = bridge_size, stab_metric = mean_sil_score_mean,
                x = "bridge size", y = "Mean silhouette score")

compare_methods(mofa_data = mofa_incomp_ref_15_summ, mofa_param = bridge_size, mofa_metric = knn_acc_bal_mean,
                stab_data = stab_incomp_ref_summ, stab_param = bridge_size, stab_metric = knn_acc_bal_mean,
                x = "bridge size", y = "Celltype accuracy")

compare_methods(mofa_data = mofa_incomp_ref_15_summ, mofa_param = bridge_size, mofa_metric = rmse_ATAC_mean,
                stab_data = stab_incomp_ref_summ, stab_param = bridge_size, stab_metric = rmse_ATAC_mean,
                x = "bridge size", y = "RMSE ATAC")

compare_methods(mofa_data = mofa_incomp_ref_15_summ, mofa_param = bridge_size, mofa_metric = rmse_RNA_mean,
                stab_data = stab_incomp_ref_summ, stab_param = bridge_size, stab_metric = rmse_RNA_mean,
                x = "bridge size", y = "RMSE RNA")
```

The MOFA method typically produces more accurate results than StabMap, with the exception of the RNA-RMSE and celltype accuracy metric. When the bridge length is relatively short (56 and up to 56, respectively), StabMap demonstrates superior performances in these cases.
